{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'chessData.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing vals per column:\\n\", data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers beyond a threshold\n",
    "# this code: keep only evaluations within Â±5000 centipawns. maybe double check where we should cut off outliers\n",
    "# data = data[(data['Evaluation'] > -5000) & (data['Evaluation'] < 5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Evaluation'] = pd.to_numeric(data['Evaluation'], errors='coerce')\n",
    "\n",
    "max_eval = data['Evaluation'].max()\n",
    "min_eval = data['Evaluation'].min()\n",
    "\n",
    "print(f\"Maximum Evaluation: {max_eval}\")\n",
    "print(f\"Minimum Evaluation: {min_eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['Evaluation'], bins=50, kde=True)\n",
    "plt.xlabel('Evaluation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Evaluation Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_values = data[pd.to_numeric(data['Evaluation'], errors='coerce').isna()]\n",
    "print(\"Non-numeric values in Evaluation column:\")\n",
    "print(non_numeric_values[['Evaluation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # means forced checkmate, could replace them with a really high positive score?:\n",
    "\n",
    "# Replace '#+X' with a large positive value and '#-X' with a large negative value\n",
    "data['Evaluation'] = data['Evaluation'].replace(\n",
    "    {r'^\\#\\+.*': '10000', r'^\\#\\-.*': '-10000'}, regex=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "non_numeric_values = data[pd.to_numeric(data['Evaluation'], errors='coerce').isna()]\n",
    "print(\"Non-numeric values in Evaluation column:\")\n",
    "print(non_numeric_values[['Evaluation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# ! pip install torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from itertools import islice\n",
    "from torch.utils.data import DataLoader\n",
    "from FEN_to_vector import to_vector\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): The DataFrame containing the FEN strings and target evaluations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the FEN string and target evaluation\n",
    "        fen = self.dataframe.iloc[idx]['FEN']\n",
    "        evaluation = self.dataframe.iloc[idx]['Evaluation']\n",
    "\n",
    "        # Use the 790-dimensional vector from FEN_to_vector\n",
    "        board_vector = to_vector(fen)\n",
    "\n",
    "        # Convert FEN to tensor (already in vector form)\n",
    "        board_tensor = torch.tensor(board_vector, dtype=torch.float32)\n",
    "\n",
    "        return board_tensor, torch.tensor(float(evaluation), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portioned_train_data = train_data.sample(frac=0.3, random_state=42)\n",
    "portioned_val_data = val_data.sample(frac=0.3, random_state=42)\n",
    "portioned_test_data = test_data.sample(frac=0.3, random_state=42)\n",
    "\n",
    "train_dataset = ChessDataset(portioned_train_data)\n",
    "val_dataset = ChessDataset(portioned_val_data)\n",
    "test_dataset = ChessDataset(portioned_test_data)\n",
    "\n",
    "batch = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(train_loader)\n",
    "\n",
    "print(f\"Total number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessNN, self).__init__()\n",
    "        # Define the fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(790, 512),  # Input layer (790 -> 512)\n",
    "            nn.ReLU(),            # ReLU activation\n",
    "            nn.Linear(512, 256),  # Hidden layer (512 -> 256)\n",
    "            nn.ReLU(),            # ReLU activation\n",
    "            nn.Linear(256, 1)     # Output layer (256 -> 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for me because I have a macbook :'(\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model = ChessNN().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001) # initial learning rate 0.001\n",
    "\n",
    "lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_limit = 100000  # only process 10 batches \n",
    "\n",
    "\n",
    "model.eval()\n",
    "val_predictions_before = []\n",
    "val_targets_before = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (board_tensors, targets) in enumerate(val_loader):\n",
    "        if i >= batch_limit:\n",
    "            break  # Stop after processing 'batch_limit' batches\n",
    "\n",
    "        board_tensors, targets = board_tensors.to(device), targets.to(device)\n",
    "        outputs = model(board_tensors)\n",
    "\n",
    "        val_predictions_before.extend(outputs.squeeze().cpu().numpy())  # Store predictions\n",
    "        val_targets_before.extend(targets.cpu().numpy())  # Store true targets\n",
    "\n",
    "# Compute Mean Squared Error before training\n",
    "mse_val_before = mean_squared_error(val_targets_before, val_predictions_before)\n",
    "print(f\"MSE before training: {mse_val_before:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50  # Set to 1 for a quick test\n",
    "batch_limit = 500 #100000  \n",
    "losses = []\n",
    "learning_rates = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  \n",
    "    total_loss = 0\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    for i, (board_tensors, targets) in enumerate(train_loader):\n",
    "        if i >= batch_limit:\n",
    "            break\n",
    "\n",
    "        board_tensors, targets = board_tensors.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(board_tensors)\n",
    "        loss = loss_fn(predictions.squeeze(), targets)\n",
    "        # losses.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    losses.append(total_loss / batch_limit)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / batch_limit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To run with entire data set for 10 epochs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run with entire data set for 10 epochs:\n",
    "\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for board_tensors, targets in train_loader:\n",
    "#         # Move data to GPU if available\n",
    "#         board_tensors, targets = board_tensors.to(device), targets.to(device)\n",
    "\n",
    "#         # Ensure board_tensors has the right shape\n",
    "#         if board_tensors.dim() == 3:\n",
    "#             board_tensors = board_tensors.unsqueeze(0)\n",
    "\n",
    "#         # Forward pass\n",
    "#         predictions = model(board_tensors)\n",
    "#         loss = loss_fn(predictions.squeeze(), targets)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_per_epoch = [\n",
    "#     np.mean(losses[i * batch_limit:(i + 1) * batch_limit])\n",
    "#     for i in range(epochs)\n",
    "# ]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color='tab:blue')\n",
    "ax1.plot(range(epochs), losses, color='tab:blue', label='Loss')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Learning Rate', color='tab:red')\n",
    "ax2.plot(range(epochs),learning_rates,color='tab:red',label='Learning Rate')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "plt.title('Loss over Training Epochs')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to consider how far off the score is rather than if it is exactly right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (board_tensors, targets) in enumerate(val_loader):\n",
    "        if i >= batch_limit:\n",
    "            break  # Stop after processing 'batch_limit' batches\n",
    "\n",
    "        board_tensors, targets = board_tensors.to(device), targets.to(device)\n",
    "        outputs = model(board_tensors)\n",
    "\n",
    "        val_predictions.extend(outputs.squeeze().cpu().numpy())  # Store predictions\n",
    "        val_targets.extend(targets.cpu().numpy())  # Store true targets\n",
    "\n",
    "# Compute Mean Squared Error after evaluation\n",
    "mse_val = mean_squared_error(val_targets, val_predictions)\n",
    "print(f\"Validation MSE: {mse_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "val_loss = 0\n",
    "batch_limit = 300  # Set the batch limit for validation\n",
    "\n",
    "with torch.no_grad():  # No gradients are calculated during evaluation\n",
    "    for i, (board_tensors, targets) in enumerate(val_loader):\n",
    "        if i >= batch_limit:\n",
    "            break\n",
    "\n",
    "        board_tensors, targets = board_tensors.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(board_tensors)\n",
    "        loss = loss_fn(predictions.squeeze(), targets)\n",
    "        val_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / batch_limit:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math156",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
